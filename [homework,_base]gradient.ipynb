{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.8"
    },
    "colab": {
      "name": "[homework, base]gradient.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rugihub/deep_learning_2018-19/blob/master/%5Bhomework%2C_base%5Dgradient.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "KxIquH6GK7kt"
      },
      "source": [
        "<p style=\"align: center;\"><img align=center src=\"https://s8.hostingkartinok.com/uploads/images/2018/08/308b49fcfbc619d629fe4604bceb67ac.jpg\" width=500 height=450/></p>\n",
        "\n",
        "<h3 style=\"text-align: center;\"><b>\"Глубокое обучение\". Базовый поток</b></h3>\n",
        "\n",
        "<h2 style=\"text-align: center;\"><b>Домашнее задание 3. Производная, градиент и градиентный спуск.\n",
        "</b></h2>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MIzAFQsnvod2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from copy import copy, deepcopy\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v6JiQgamvod9",
        "colab_type": "text"
      },
      "source": [
        "## Задание 1 ##"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y38bKm9Tvod-",
        "colab_type": "text"
      },
      "source": [
        "__Напоминание:__\n",
        "Если вдруг вы не помните определение производной, то вот оно:\n",
        "\n",
        "$$\\lim_{h→0}\\frac{f(x+h)−f(x)}{h}$$\n",
        "Или, что то же:\n",
        "$$\\lim_{x→x_0}\\frac{f(x)−f(x_0)}{x-x_0}$$\n",
        "\n",
        "\n",
        "Если такой предел существует, то и производная существует (и равна этому пределу).\n",
        "\n",
        "Какие из перечисленных функций имеют производную в нуле $(x_0 = 0)$?\n",
        "\n",
        "1) $f(x) = |x|^2$\n",
        "\n",
        "2) $f(x) = \\frac{sin(x)}{x}$\n",
        "\n",
        "3) $f(x) = |x|$\n",
        "\n",
        "4) $f(x) = \n",
        "     \\begin{cases}\n",
        "       x^2 &\\text{$x \\ne 0$}\\\\\n",
        "       0 &\\text{$x = 0$}\n",
        "     \\end{cases}$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mfdVpqeTvoeA",
        "colab_type": "text"
      },
      "source": [
        "**Ответ:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nc4hJhHuvoeC",
        "colab_type": "text"
      },
      "source": [
        "## Задание 2 ##"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D_MN9myzvoeD",
        "colab_type": "text"
      },
      "source": [
        "Посчитайте производную $f(x)=x^x$ в точке $x_0 = e$\n",
        "\n",
        "Тут надо действовать хитро. По определению взять не получится.\n",
        "\n",
        "Ответ округлите до десятых."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rBcFA6ErvoeF",
        "colab_type": "text"
      },
      "source": [
        "**Ответ:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WsJ_W3VXvoeH",
        "colab_type": "text"
      },
      "source": [
        "## Задание 3 ##"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sj-eLSJyvoeI",
        "colab_type": "text"
      },
      "source": [
        "Вычислите производную $f(x)=tg(x)⋅ln(cos(x^2)+1)$, в точке $x_0 = 0$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ctY73lF1voeK",
        "colab_type": "text"
      },
      "source": [
        "**Ответ:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vHjndqfCvoeL",
        "colab_type": "text"
      },
      "source": [
        "## Задание 4 ##"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ve1wPT74voeN",
        "colab_type": "text"
      },
      "source": [
        "Предположим, у вас есть функция, и вы бы хотели знать ее производную, но у вас нет аналитического выражения для нее.\n",
        "\n",
        "Ваше задание --- написать функцию, которая будет вычислять производную ... *кхм* функции. Тавтология --- это не очень приятно, но я думаю, что Вы поняли что нужно делать.\n",
        "\n",
        "Однако не подумайте, что вас просят написать что-то, что будет вычислять эту самую производную аналитически. \n",
        "\n",
        "Попробуйте это сделать **приближенно**, опираясь на определение производной."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tYuZF08nvoeO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def numerical_derivative_1d(func, epsilon):\n",
        "    \"\"\"\n",
        "    Функция для приближённого вычисления производной функции одной переменной. \n",
        "    :param func: float -> float — произвольная дифференцируемая функция\n",
        "    :param epsilon: float — максимальная величина приращения по оси Ох\n",
        "    :return: другая функция, которая приближённо вычисляет производную в точке\n",
        "    \"\"\"\n",
        "    def deriv_func(x):\n",
        "        \"\"\"\n",
        "        :param x: float — точка, в которой нужно вычислить производную\n",
        "        :return: приближённое значение производной в этой точке\n",
        "        \"\"\"\n",
        "        # YOUR CODE\n",
        "    return deriv_func"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ogo1thL-voeT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Проверьте себя!\n",
        "def polynom_to_prime(x):\n",
        "    return 20 * x**5 + x**3 - 5 * x**2 + 2 * x + 2.0\n",
        "\n",
        "\n",
        "def primed_poly(x):\n",
        "    return 100 * x**4 + 3 * x**2 -10 * x + 2.0\n",
        "\n",
        "\n",
        "approx_deriv = numerical_derivative_1d(polynom_to_prime, 1e-5)\n",
        "\n",
        "grid = np.linspace(-2, 2, 100)\n",
        "right_flag = True\n",
        "tol = 0.05\n",
        "debug_print = []\n",
        "\n",
        "for x in grid:\n",
        "    estimation_error = abs(primed_poly(x) - approx_deriv(x)) \n",
        "    if estimation_error > tol:\n",
        "        debug_print.append((estimation_error, primed_poly(x), approx_deriv(x)))\n",
        "        right_flag = False\n",
        "\n",
        "if not right_flag:\n",
        "    print(\"Что-то не то...\")\n",
        "    print(debug_print)\n",
        "    plt.plot(grid, primed_poly(grid), label=\"Истинная производная\")\n",
        "    plt.plot(grid, approx_deriv(grid), label=\"Численное приближение\")\n",
        "    plt.legend()\n",
        "\n",
        "print(str(right_flag))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oJVDE5FovoeZ",
        "colab_type": "text"
      },
      "source": [
        "## Задание 5 ##"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "91UYo01wvoea",
        "colab_type": "text"
      },
      "source": [
        "В этом задании Вы должны найти минимум функций с помощью градиентного спуска.\n",
        "\n",
        "Вам на вход подаются функция `func`, ее производная `deriv` **(*)**, а также начальная точка `start`, на выходе - точка локального минимума. Для вашего удобства мы написали функцию для отрисовки траектории градиентного спуска\n",
        "\n",
        "**(*)** - вам не нужно будет ее вычислять. То, что вы написали в предыдущем задании, вам пригодится чуть позже."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kwZyc1vVd3h9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def plot_convergence_1d(func, x_steps, y_steps, ax, grid=None, title=\"\"):\n",
        "    \"\"\"\n",
        "    Функция отрисовки шагов градиентного спуска. \n",
        "    Не меняйте её код без необходимости! \n",
        "    :param func: функция, которая минимизируется градиентным спуском\n",
        "    :param x_steps: np.array(float) — шаги алгоритма по оси Ox\n",
        "    :param y_steps: np.array(float) — шаги алгоритма по оси Оу\n",
        "    :param ax: холст для отрисовки графика\n",
        "    :param grid: np.array(float) — точки отрисовки функции func\n",
        "    :param title: str — заголовок графика\n",
        "    \"\"\"\n",
        "    ax.set_title(title, fontsize=16, fontweight=\"bold\")\n",
        "\n",
        "    if grid is None:\n",
        "        grid = np.linspace(np.min(x_steps), np.max(x_steps), 100)\n",
        "\n",
        "    fgrid = func(grid)\n",
        "    ax.plot(grid, fgrid)\n",
        "    yrange = np.max(fgrid) - np.min(fgrid)\n",
        "\n",
        "    arrow_kwargs = dict(linestyle=\"--\", color=\"grey\", alpha=0.4)\n",
        "    for i, _ in enumerate(x_steps):\n",
        "        if i + 1 < len(x_steps):\n",
        "            ax.arrow(\n",
        "                x_steps[i], y_steps[i], \n",
        "                x_steps[i + 1] - x_steps[i],\n",
        "                y_steps[i + 1] - y_steps[i], \n",
        "                **arrow_kwargs\n",
        "            )\n",
        "\n",
        "    n = len(x_steps)\n",
        "    color_list = [(i / n, 0, 0, 1 - i / n) for i in range(n)]\n",
        "    ax.scatter(x_steps, y_steps, c=color_list)\n",
        "    ax.scatter(x_steps[-1], y_steps[-1], c=\"red\")\n",
        "    ax.set_xlabel(r\"$x$\")\n",
        "    ax.set_ylabel(r\"$y$\")\n",
        "\n",
        "\n",
        "class LoggingCallback:\n",
        "    \"\"\"\n",
        "    Класс для логирования шагов градиентного спуска. \n",
        "    Сохраняет точку (x, f(x)) на каждом шаге.\n",
        "    Пример использования в коде: callback(x, f(x))\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        self.x_steps = []\n",
        "        self.y_steps = []\n",
        "\n",
        "    def __call__(self, x, y):\n",
        "        self.x_steps.append(x)\n",
        "        self.y_steps.append(y)\n",
        "\n",
        "\n",
        "def test_convergence_1d(grad_descent, test_cases, tol=1e-2, axes=None, grid=None):\n",
        "    \"\"\"\n",
        "    Функция для проверки корректности вашего решения в одномерном случае.\n",
        "    Она же используется в тестах на Stepik, так что не меняйте её код!\n",
        "    :param grad_descent: ваша реализация градиентного спуска\n",
        "    :param test_cases: dict(dict), тесты в формате dict с такими ключами:\n",
        "        - \"func\" — функция (обязательно)\n",
        "        - \"deriv\" — её производная (обязательно)\n",
        "        - \"start\" — начальная точка start (м.б. None) (опционально) \n",
        "        - \"low\", \"high\" — диапазон для выбора начальной точки (опционально)\n",
        "        - \"answer\" — ответ (обязательно)\n",
        "    При желании вы можете придумать и свои тесты.\n",
        "    :param tol: предельное допустимое отклонение найденного ответа от истинного\n",
        "    :param axes: матрица холстов для отрисовки, по ячейке на тест\n",
        "    :param grid: np.array(float), точки на оси Ох для отрисовки тестов\n",
        "    :return: флаг, корректно ли пройдены тесты, и дебажный вывод в случае неудачи\n",
        "    \"\"\"\n",
        "    right_flag = True\n",
        "    debug_log = []\n",
        "    for i, key in enumerate(test_cases.keys()):\n",
        "        # Формируем входные данные и ответ для алгоритма.\n",
        "        answer = test_cases[key][\"answer\"]\n",
        "        test_input = deepcopy(test_cases[key])\n",
        "        del test_input[\"answer\"]\n",
        "        # Запускаем сам алгоритм.\n",
        "        callback = LoggingCallback()  # Не забываем про логирование\n",
        "        res_point = grad_descent(**test_input, callback=callback)\n",
        "        # Отрисовываем результаты.\n",
        "        if axes is not None:\n",
        "            ax = axes[np.unravel_index(i, shape=axes.shape)]\n",
        "            x_steps = np.array(callback.x_steps)\n",
        "            y_steps = np.array(callback.y_steps)\n",
        "            plot_convergence_1d(\n",
        "                test_input[\"func\"], x_steps, y_steps, \n",
        "                ax, grid, key\n",
        "            )\n",
        "            ax.axvline(answer, 0, linestyle=\"--\", c=\"red\",\n",
        "                        label=f\"true answer = {answer}\")\n",
        "            ax.axvline(x_steps[-1], 0, linestyle=\"--\", c=\"xkcd:tangerine\", \n",
        "                        label=f\"estimate = {np.round(x_steps[-1], 3)}\")\n",
        "            ax.legend(fontsize=16)\n",
        "        # Проверяем, что найдення точка достаточно близко к истинной\n",
        "        if abs(answer - res_point) > tol:\n",
        "            debug_log.append(\n",
        "                f\"Тест '{key}':\\n\"\n",
        "                f\"\\t- ответ: {answer}\\n\"\n",
        "                f\"\\t- вывод алгоритма: {res_point}\"\n",
        "            )\n",
        "            right_flag = False\n",
        "    return right_flag, debug_log"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sfBgQXzxfKRk",
        "colab_type": "text"
      },
      "source": [
        "На каждой итерации вызывайте `callback(x, f(x))`, где `x` это результат шага градиентного спуска.   \n",
        "Это нужно для отрисовки шагов алгоритма."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X2B6AZ_yvoec",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def grad_descent_v1(func, deriv, start=None, callback=None):\n",
        "    \"\"\" \n",
        "    Реализация градиентного спуска для функций с одним локальным минимумом,\n",
        "    совпадающим с глобальным. Все тесты будут иметь такую природу.\n",
        "    :param func: float -> float — функция \n",
        "    :param deriv: float -> float — её производная\n",
        "    :param start: float — начальная точка\n",
        "    \"\"\"\n",
        "    if start is None:\n",
        "        # Если точка не дана, сгенерируем случайную\n",
        "        # из стандартного нормального распределения.\n",
        "        # При таком подходе начальная точка может быть\n",
        "        # любой, а не только из какого-то ограниченного диапазона\n",
        "        np.random.seed(179)\n",
        "        start = np.random.randn()\n",
        "\n",
        "    estimate = start\n",
        "    callback(estimate, func(estimate))  # не забывайте логировать шаги!\n",
        "\n",
        "    # YOUR CODE\n",
        "    return estimate"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1eOO6t0cvoeh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_cases = {\n",
        "    \"square\": {\n",
        "        \"func\" : lambda x: x * x, \n",
        "        \"deriv\" : lambda x: 2 * x, \n",
        "        \"start\" : 2, \n",
        "        \"answer\" : 0.0\n",
        "    },\n",
        "    \"module\": {\n",
        "        \"func\" : lambda x: abs(x),  \n",
        "        \"deriv\" : lambda x: 1 if x > 0 else -1,\n",
        "        \"start\" : 2, \n",
        "        \"answer\" : 0.0\n",
        "    },\n",
        "    \"fourth_power\": {\n",
        "        \"func\" : lambda x: (x - 1)**4, \n",
        "        \"deriv\" : lambda x: 4 * (x - 1)**3, \n",
        "        \"start\" : -1, \n",
        "        \"answer\" : 1.0\n",
        "    },\n",
        "    \"ln_x2_1\": {\n",
        "        \"func\" : lambda x: np.log((x + 1)**2 + 1),  \n",
        "        \"deriv\" : lambda x: 2 * (x + 1) / (x**2 +1), \n",
        "        \"start\" : 1, \n",
        "        \"answer\" : -1.0\n",
        "    }\n",
        "}\n",
        "\n",
        "tol = 1e-4  # желаемая точность \n",
        "fig, axes = plt.subplots(2, 2, figsize=(14, 12))\n",
        "fig.suptitle(\"Градиентный спуск, версия 1\", fontweight=\"bold\", fontsize=20)\n",
        "grid = np.linspace(-2, 2, 100)\n",
        "\n",
        "is_correct, debug_log = test_convergence_1d(\n",
        "    grad_descent_v1, test_cases, tol, \n",
        "    axes, grid\n",
        ")\n",
        "if not is_correct:\n",
        "    print(\"Не сошлось. Дебажный вывод:\")\n",
        "    for log_entry in debug_log:\n",
        "        print(log_entry)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bjYYcxG0voel",
        "colab_type": "text"
      },
      "source": [
        "## Задание 6 ##"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Bqka1-1voen",
        "colab_type": "text"
      },
      "source": [
        "Это задание чуть сложнее. Если раньше Вам нужно было просто найти минимум у довольно хорошей функции, то сейчас в тестах будут плохие. У них будет несколько минимумов и вам нужно найти глобальный у каждой функции.\n",
        "\n",
        "В общем случае такая задача невыполнима, но у вас будут одномерные функции и все самое интересное будет сосредоточено в районе нуля. Скажем, глобальный минимум будет лежать в пределах `(-3, 3)`. Вам нужно как-то изменить градиентный спуск, который вы написали в предыдущем задании, чтобы он работал и в таком случае.\n",
        "\n",
        "И снова не забывайте вызывать `callback(x, f(x))` на каждом шаге алгоритма!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LD06vwnQvoep",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def grad_descent_v2(func, deriv, low=None, high=None, callback=None):\n",
        "    \"\"\" \n",
        "    Реализация градиентного спуска для функций с несколькими локальным минимумами,\n",
        "    но с известной окрестностью глобального минимума. \n",
        "    Все тесты будут иметь такую природу.\n",
        "    :param func: float -> float — функция \n",
        "    :param deriv: float -> float — её производная\n",
        "    :param low: float — левая граница окрестности\n",
        "    :param high: float — правая граница окрестности\n",
        "    \"\"\"\n",
        "    # YOUR CODE\n",
        "    return best_estimate"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VgXx51m4voeu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_cases = {\n",
        "    \"poly1\" : {\n",
        "        \"func\" : lambda x: x**4 + 3 * x**3 + x**2 - 1.5 * x + 1,\n",
        "        \"deriv\" : lambda x: 4 * x**3 + 9 * x**2 + 2 * x - 1.5,\n",
        "        \"low\" : -3, \"high\" : 3, \"answer\" : -1.88\n",
        "    },\n",
        "    \"poly2\" : {\n",
        "        \"func\" : lambda x: x**4 + 3 * x**3 + x**2 - 2 * x + 1.0,\n",
        "        \"deriv\" : lambda x: 4 * x**3 + 9 * x**2 + 2 * x - 2.0, \n",
        "        \"low\" : -3, \"high\" : 3, \"answer\" : 0.352\n",
        "    },\n",
        "    \"poly3\" : {\n",
        "        \"func\" : lambda x: x**6 + 3 * x**3 + x**2 - 2 * x + 1.0,\n",
        "        \"deriv\" : lambda x: 6 * x**5 + 9 * x **2 + 2 * x - 2.0, \n",
        "        \"low\" : -3, \"high\" : 3, \"answer\" : 0.368\n",
        "    }\n",
        "}\n",
        "tol = 1e-2 # желаемая точность\n",
        "\n",
        "fig, axes = plt.subplots(1, 3, figsize=(24, 8))\n",
        "fig.suptitle(\"Градиентный спуск, версия 2\", fontweight=\"bold\", fontsize=20)\n",
        "grid = np.linspace(-3, 3, 100)\n",
        "\n",
        "is_correct, debug_log = test_convergence_1d(\n",
        "    grad_descent_v2, test_cases, tol, \n",
        "    axes, grid\n",
        ")\n",
        "\n",
        "if not is_correct:\n",
        "    print(\"Не сошлось. Дебажный вывод:\")\n",
        "    for log_entry in debug_log:\n",
        "        print(log_entry)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pxQWFDuovoe-",
        "colab_type": "text"
      },
      "source": [
        "## Задание 7 ##"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8-8n8ZzIvoe_",
        "colab_type": "text"
      },
      "source": [
        "__Напоминание:__\n",
        "Если вдруг вы не знаете или не помните дифференциальное исчисление функций многих переменных, то вот несколько ключевых определений, которые помогут вам.справиться с заданиями 7-9. Здесь и ниже рассматриваются скалярные функции многих переменных, т.е. $f: U \\to \\mathbb{R} $, где $U$ — область в $\\mathbb{R}^{n}$. \n",
        "\n",
        "__Определение: Производная функции многих переменных__ \n",
        "\n",
        "$$\\lim_{\\|h\\|→0}\\frac{f(x_0+h)−f(x_0)}{\\|h\\|}$$\n",
        "Или, что то же:\n",
        "$$\\lim_{x→x_0}\\frac{f(x)−f(x_0)}{\\|x-x_0\\|}$$\n",
        "\n",
        "Если такой предел существует, то и производная в точке $x_0$ существует (и равна этому пределу).\n",
        "\n",
        "__Определение: Производная по направлению__ \n",
        "\n",
        "Пусть зафиксирован единичный вектор $v$. Тогда если в точке $x_0$ существует предел\n",
        "$$\\lim_{\\alpha→0}\\frac{f(x_0+\\alpha v)−f(x_0)}{\\alpha}$$\n",
        "то он называется производной по направлению $v$ в этой точке. Обозначается через $\\nabla_{v} f(x_0)$.\n",
        "\n",
        "__Определение: Частная производная__\n",
        "\n",
        "Производная по направлению, заданному одним из базисных векторов $\\langle e_1, \\ldots, e_n \\rangle$.  \n",
        "Частная производная по направлению $e_i$ обозначается через $\\frac{\\partial f}{\\partial x_i}$.\n",
        "\n",
        "__Определение: Градиент__\n",
        "\n",
        "Ковектор, составленный из частных производных\n",
        "$\\nabla f := \\left( \\frac{\\partial f}{\\partial x_1}, \\ldots, \\frac{\\partial f}{\\partial x_n} \\right) $\n",
        "\n",
        "---\n",
        "\n",
        "В лекции было несколько функций, чьи градиенты Вам было предложено вычислить.\n",
        "\n",
        "Вычислите градиент следующей функции:\n",
        "\n",
        "$\\psi(x,y,z) = sin(xz) - y^2z + e^x$\n",
        "\n",
        "Запишите ответ в виде строки \"$(\\psi_{x}^{'})*i + (\\psi_{y}^{'})*j + (\\psi_{z}^{'})*k$\", где вместо $\\psi_{x}^{'}, \\psi_{y}^{'}, \\psi_{z}^{'}$  подставьте вычисленные частные производные (компоненты градиента). \n",
        "\n",
        "Можно пользоваться символами \"+\", \"-\", \"*\", \"/\", \"^\", \"sin\", \"cos\", \")\", \"(\", \"e\". \n",
        "\n",
        "Примеры записи формул можно увидеть в ячейках текста задания."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Q47B6rkvofB",
        "colab_type": "text"
      },
      "source": [
        "**Ответ:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JZ3JJG_mvofD",
        "colab_type": "text"
      },
      "source": [
        "## Задание 8 ##"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vn9o18L5vofE",
        "colab_type": "text"
      },
      "source": [
        "Еще один градиент, похожий на тот, что был на лекции:\n",
        "\n",
        "$\\psi(x,y,z) = ln(cos(e^{x+y})) - ln(xy)$ \n",
        "\n",
        "Запишите ответ в виде строки \"$(\\psi_{x}^{'})*i + (\\psi_{y}^{'})*j + (\\psi_{z}^{'})*k$\", где вместо $\\psi_{x}^{'}, \\psi_{y}^{'}, \\psi_{z}^{'}$  подставьте вычисленные частные производные (компоненты градиента). \n",
        "\n",
        "Можно пользоваться символами \"+\", \"-\", \"*\", \"/\", \"^\", \"sin\", \"cos\", \")\", \"(\", \"e\". \n",
        "\n",
        "Примеры записи формул можно увидеть в ячейках текста задания.\n",
        "\n",
        "Запишите ответ в виде строки \"()*i + ()*j\", где вместо ,  подставьте вычисленные производные (компоненты градиента). Можно пользоваться символами \"+\", \"-\", \"*\", \"/\", \"^\", \"sin\", \"cos\", \")\", \"(\", \"e\", \"ln\", \"tg\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wnAshrSWvofF",
        "colab_type": "text"
      },
      "source": [
        "**Ответ:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rJi66t39voey",
        "colab_type": "text"
      },
      "source": [
        "## Задание 9 ##"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L96np6shvoe0",
        "colab_type": "text"
      },
      "source": [
        "А теперь все вместе!\n",
        "\n",
        "У вас есть только функция, которую Вам отдают в качестве аргумента и вы должны найти её минимум.\n",
        "\n",
        "Вы будете искать глобальный, у вас это должно получиться лишь потому что тут они хорошие.\n",
        "\n",
        "Да, и еще, теперь они не одномерные, а двумерные.\n",
        "\n",
        "Минимум нужно искать где-то на $\\Omega = (-5, 5) \\times (-5, 5)$\n",
        "\n",
        "***Подсказка*** можете использовать следующие параметры:\n",
        "\n",
        "* Точность при вычислении производной $dx = 10^{-10}$\n",
        "* Критерий остановки - близость к ответу $\\delta = 10^{-10}$ и кол-во итераций $10^4$\n",
        "* Длина шага градиентного спуска $lr = 1$\n",
        "\n",
        "И вновь мы предоставляем функцию отрисовки шагов для пущего удобства."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ARnTlYWcLGo0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def plot_convergence_2d(func, steps, ax, xlim, ylim, cmap=\"viridis\", title=\"\"):\n",
        "    \"\"\"\n",
        "    Функция отрисовки шагов градиентного спуска. \n",
        "    Не меняйте её код без необходимости! \n",
        "    :param func: функция, которая минимизируется градиентным спуском\n",
        "    :param steps: np.array(), N x 2 — шаги алгоритма\n",
        "    :param ax: холст для отрисовки графика\n",
        "    :param xlim: tuple(float), 2 — диапазон по первой оси\n",
        "    :param ylim: tuple(float), 2 — диапазон по второй оси\n",
        "    :param cmap: str — название палитры\n",
        "    :param title: str — заголовок графика\n",
        "    \"\"\"\n",
        "\n",
        "    ax.set_title(title, fontsize=20, fontweight=\"bold\")\n",
        "    # Отрисовка значений функции на фоне\n",
        "    xrange = np.linspace(*xlim, 100)\n",
        "    yrange = np.linspace(*ylim, 100)\n",
        "    grid = np.meshgrid(xrange, yrange)\n",
        "    X, Y = grid\n",
        "    fvalues = func(\n",
        "        np.dstack(grid).reshape(-1, 2)\n",
        "    ).reshape((xrange.size, yrange.size))\n",
        "    ax.pcolormesh(xrange, yrange, fvalues, cmap=cmap, alpha=0.8)\n",
        "    CS = ax.contour(xrange, yrange, fvalues)\n",
        "    ax.clabel(CS, CS.levels, inline=True)\n",
        "    # Отрисовка шагов алгоритма в виде стрелочек\n",
        "    arrow_kwargs = dict(linestyle=\"--\", color=\"black\", alpha=0.8)\n",
        "    for i, _ in enumerate(steps):\n",
        "        if i + 1 < len(steps):\n",
        "            ax.arrow(\n",
        "                *steps[i],\n",
        "                *(steps[i+1] - steps[i]),\n",
        "                **arrow_kwargs\n",
        "            )\n",
        "    # Отрисовка шагов алгоритма в виде точек\n",
        "    n = len(steps)\n",
        "    color_list = [(i / n, 0, 0, 1 - i / n) for i in range(n)]\n",
        "    ax.scatter(steps[:, 0], steps[:, 1], c=color_list, zorder=10)\n",
        "    ax.scatter(steps[-1, 0], steps[-1, 1], \n",
        "               color=\"red\", label=f\"estimate = {np.round(steps[-1], 2)}\")\n",
        "    # Финальное оформление графиков\n",
        "    ax.set_xlim(xlim)\n",
        "    ax.set_ylim(ylim)\n",
        "    ax.set_ylabel(\"$y$\")\n",
        "    ax.set_xlabel(\"$x$\")\n",
        "    ax.legend(fontsize=16)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "egm2nGHCvoe1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def numerical_derivative_2d(func, epsilon):\n",
        "    \"\"\"\n",
        "    Функция для приближённого вычисления градиента функции двух переменных. \n",
        "    :param func: np.ndarray -> float — произвольная дифференцируемая функция\n",
        "    :param epsilon: float — максимальная величина приращения по осям\n",
        "    :return: другая функция, которая приближённо вычисляет градиент в точке\n",
        "    \"\"\"\n",
        "    def grad_func(x):\n",
        "        \"\"\"\n",
        "        :param x: np.ndarray — точка, в которой нужно вычислить производную\n",
        "        :return: приближённое значение производной в этой точке\n",
        "        \"\"\"\n",
        "        # YOUR CODE\n",
        "    return grad_func\n",
        "\n",
        "\n",
        "def grad_descent_2d(func, low, high, callback=None):\n",
        "    \"\"\" \n",
        "    Реализация градиентного спуска для функций двух переменных \n",
        "    с несколькими локальным минимумами, но известной квадратной окрестностью\n",
        "    глобального минимума. Все тесты будут иметь такую природу.\n",
        "\n",
        "    Обратите внимание, что здесь градиент функции не дан.\n",
        "    Его нужно вычислять приближённо.\n",
        "\n",
        "    :param func: np.ndarray -> float — функция \n",
        "    :param low: левая граница интервала по каждой из осей\n",
        "    :param high: правая граница интервала по каждой из осей\n",
        "    \"\"\"\n",
        "\n",
        "    # YOUR CODE\n",
        "    return best_estimate\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UFgjqiYZvoe5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def test_convergence_2d(grad_descent_2d, test_cases, tol, axes=None):\n",
        "    \"\"\"\n",
        "    Функция для проверки корректности вашего решения в двумерном случае.\n",
        "    Она же используется в тестах на Stepik, так что не меняйте её код!\n",
        "    :param grad_descent_2d: ваша реализация градиентного спуска\n",
        "    :param test_cases: dict(dict), тесты в формате dict с такими ключами:\n",
        "        - \"func\" — функция \n",
        "        - \"deriv\" — её производная \n",
        "        - \"low\", \"high\" — диапазон для выбора начальной точки \n",
        "        - \"answer\" — ответ \n",
        "    При желании вы можете придумать и свои тесты.\n",
        "    :param tol: предельное допустимое отклонение найденного ответа от истинного\n",
        "    :param axes: матрица холстов для отрисовки, по ячейке на тест\n",
        "    :return: флаг, корректно ли пройдены тесты, и дебажный вывод в случае неудачи\n",
        "    \"\"\"\n",
        "    right_flag = True\n",
        "    debug_log = []\n",
        "    for i, key in enumerate(test_cases.keys()):\n",
        "        # Формируем входные данные и ответ для алгоритма.\n",
        "        answer = test_cases[key][\"answer\"]\n",
        "        test_input = deepcopy(test_cases[key])\n",
        "        del test_input[\"answer\"]\n",
        "        # Запускаем сам алгоритм.\n",
        "        callback = LoggingCallback()  # Не забываем про логирование\n",
        "        res_point = grad_descent_2d(**test_input, callback=callback)\n",
        "        # Отрисовываем результаты.\n",
        "        if axes is not None:\n",
        "            ax = axes[np.unravel_index(i, shape=axes.shape)]\n",
        "            plot_convergence_2d(\n",
        "                np.vectorize(test_input[\"func\"], signature=\"(n)->()\"), \n",
        "                np.vstack(callback.x_steps), \n",
        "                ax=ax, \n",
        "                xlim=(test_input[\"low\"], test_input[\"high\"]), \n",
        "                ylim=(test_input[\"low\"], test_input[\"high\"]),\n",
        "                title=key\n",
        "            )   \n",
        "        # Проверяем, что найденная точка достаточно близко к истинной\n",
        "        if np.linalg.norm(answer - res_point, ord=1) > tol:\n",
        "            debug_log.append(\n",
        "                f\"Тест '{key}':\\n\"\n",
        "                f\"\\t- ответ: {answer}\\n\"\n",
        "                f\"\\t- вывод алгоритма: {res_point}\"\n",
        "            )\n",
        "            right_flag = False\n",
        "    return right_flag, debug_log\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vR_-Ucs4giUQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_cases = {\n",
        "    \"concentric_circles\" : {\n",
        "        \"func\" : lambda x: (\n",
        "            -1 / ((x[0] - 1)**2 + (x[1] - 1.5)**2 + 1)\n",
        "            * np.cos(2 * (x[0] - 1)**2 + 2 * (x[1] - 1.5)**2)\n",
        "        ),\n",
        "        \"low\" : -5,\n",
        "        \"high\" : 5,\n",
        "        \"answer\" : np.array([1, 1.5])\n",
        "    }\n",
        "}\n",
        "tol = 1e-3  # желаемая точность\n",
        "\n",
        "fig, axes = plt.subplots(figsize=(10, 10), squeeze=False)\n",
        "fig.suptitle(\"Градиентный спуск в 2D\", fontsize=25, fontweight=\"bold\")\n",
        "is_correct, debug_log = test_convergence_2d(grad_descent_2d, test_cases, tol, axes)\n",
        "\n",
        "if not is_correct:\n",
        "    print(\"Не сошлось. Дебажный вывод:\")\n",
        "    for log_entry in debug_log:\n",
        "        print(log_entry)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}